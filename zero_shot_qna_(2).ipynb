{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "THIS MODEL HAS BEEN MADE AND RUN ON GOOGLE COLAB"
      ],
      "metadata": {
        "id": "v7Q6OBaHOQya"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "install the python sdk"
      ],
      "metadata": {
        "id": "JEQprooohscN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q -U google-generativeai"
      ],
      "metadata": {
        "id": "j5Jg-R5LhwIE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df7c0d67-e058-4e7a-d872-a4b9f246fb05"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/137.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/137.4 kB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━\u001b[0m \u001b[32m92.2/137.4 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m137.4/137.4 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import generative ai package"
      ],
      "metadata": {
        "id": "xf_PHdNlh1L1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai"
      ],
      "metadata": {
        "id": "BYXk8YKph4Ou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Used to securely store your API key\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "TuQR41dPh_Kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Setup your API key"
      ],
      "metadata": {
        "id": "9nsStpptiJl8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "tSG_P1eKiKM1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instantiate the model"
      ],
      "metadata": {
        "id": "PO_6tppmPaOx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text_model = genai.GenerativeModel('gemini-pro')"
      ],
      "metadata": {
        "id": "VbK1pA3SjhqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When an LLM chatbot gives an answer to a question, the last numerical value is it's final answer (based on observation). So make a function which extracts the last numerical value"
      ],
      "metadata": {
        "id": "O5sjpPj-PWOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def extract_last_number(input_string):\n",
        "    # Split the string using regex\n",
        "    parts = re.split(r'[^a-zA-Z0-9.]', input_string)\n",
        "\n",
        "    # Initialize a variable to store the last number found\n",
        "    last_number = None\n",
        "\n",
        "    # Iterate over each part in reverse order\n",
        "    for part in reversed(parts):\n",
        "        # Attempt to convert the part to an integer or float\n",
        "        try:\n",
        "            last_number = int(part)\n",
        "            break\n",
        "        except ValueError:\n",
        "            try:\n",
        "                last_number = float(part)\n",
        "                break\n",
        "            except ValueError:\n",
        "                pass  # If the part is not a number, continue to the next part\n",
        "\n",
        "    return last_number\n"
      ],
      "metadata": {
        "id": "VqhgAzMLPW-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make a function which takes the question as an input, use prompts which will make the AI use a chain-of-thoughts method, then extract the numerical result. It will not block the question for any false-positive harm category instance."
      ],
      "metadata": {
        "id": "wO8wOsz0P8Uq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.generativeai.types import HarmCategory, HarmBlockThreshold\n",
        "def solve_question(question):\n",
        "  # Construct Chain-of-Thought prompt\n",
        "  prompt1 = f\"\"\"\n",
        "  Use the Chain-of-Thought methodology to explain your step-by-step calculations or logic.\n",
        "  Q: {question}\n",
        "  Let's think step by step.\n",
        "  \"\"\"\n",
        "  p=text_model.generate_content(prompt1,\n",
        "                                safety_settings={\n",
        "      HarmCategory.HARM_CATEGORY_HATE_SPEECH: HarmBlockThreshold.BLOCK_NONE,\n",
        "      HarmCategory.HARM_CATEGORY_HARASSMENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "      HarmCategory.HARM_CATEGORY_SEXUALLY_EXPLICIT: HarmBlockThreshold.BLOCK_NONE,\n",
        "      HarmCategory.HARM_CATEGORY_DANGEROUS_CONTENT: HarmBlockThreshold.BLOCK_NONE,\n",
        "  }).text\n",
        "  #print(p)\n",
        "  answer=extract_last_number(p)\n",
        "  if(answer==int(answer)):\n",
        "    answer=int(answer)\n",
        "  return answer\n"
      ],
      "metadata": {
        "id": "Zm7BCMCkI-Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "sometimes AI may generate a result which may not have a numeric value, or a value which overflows the assigned data range. so error handling is done"
      ],
      "metadata": {
        "id": "4nui2WfuRLfx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def solve(question):\n",
        "  try:\n",
        "    return solve_question(question)\n",
        "  except (KeyError, TypeError, ValueError, OverflowError) as e:\n",
        "    print(\"Error Handled \")\n",
        "    return solve(question)"
      ],
      "metadata": {
        "id": "K4CfDsFmRKIy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "since this model takes an extreme amount of time, the csv file containing the questions were split into two parts and run in each code cell for predicting answers"
      ],
      "metadata": {
        "id": "4quM-zRrR8iS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Read the original CSV file\n",
        "df = pd.read_csv('TestSet.csv')\n",
        "\n",
        "# Split the DataFrame into two parts of 249 rows each\n",
        "part1 = df.iloc[:249]\n",
        "part2 = df.iloc[249:]\n",
        "\n",
        "# Write each part to separate CSV files\n",
        "part1.to_csv('part1.csv', index=False)\n",
        "part2.to_csv('part2.csv', index=False)\n"
      ],
      "metadata": {
        "id": "czjHGyv7ztdd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "predicting answers for part 1"
      ],
      "metadata": {
        "id": "FR1LdkU6SONZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read the input CSV file\n",
        "input_df1 = pd.read_csv('part1.csv')\n",
        "\n",
        "# Iterate over each row in the input DataFrame\n",
        "output_data = []\n",
        "for index, row in input_df1.iterrows():\n",
        "    # Get the question from the current row\n",
        "    question = row['QUESTION']\n",
        "\n",
        "    # Call the solve function to get the answer\n",
        "    #solve(question)\n",
        "    answer = solve(question)\n",
        "\n",
        "    # Append the ID and answer to the output data list\n",
        "    output_data.append({'ID': row['ID'], 'LABEL': answer})\n",
        "    print(index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6WzCqIaYQclR",
        "outputId": "9305576c-14d2-4452-e56c-13a7b742f842"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n",
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "Error Handled \n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n",
            "202\n",
            "203\n",
            "204\n",
            "205\n",
            "206\n",
            "207\n",
            "208\n",
            "209\n",
            "210\n",
            "211\n",
            "212\n",
            "213\n",
            "214\n",
            "215\n",
            "216\n",
            "217\n",
            "218\n",
            "219\n",
            "220\n",
            "221\n",
            "222\n",
            "223\n",
            "224\n",
            "225\n",
            "226\n",
            "227\n",
            "228\n",
            "229\n",
            "230\n",
            "231\n",
            "232\n",
            "233\n",
            "234\n",
            "235\n",
            "236\n",
            "237\n",
            "238\n",
            "239\n",
            "240\n",
            "241\n",
            "242\n",
            "243\n",
            "244\n",
            "245\n",
            "246\n",
            "247\n",
            "248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "predicting answers for part 2"
      ],
      "metadata": {
        "id": "bTZf5f0nSSsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "input_df2 = pd.read_csv('part2.csv')\n",
        "\n",
        "# Iterate over each row in the input DataFrame\n",
        "for index, row in input_df2.iterrows():\n",
        "    # Get the question from the current row\n",
        "    question = row['QUESTION']\n",
        "\n",
        "    # Call the solve function to get the answer\n",
        "    #solve(question)\n",
        "    answer = solve(question)\n",
        "    if(answer==int(answer)):\n",
        "      answer=int(answer)\n",
        "\n",
        "    # Append the ID and answer to the output data list\n",
        "    output_data.append({'ID': row['ID'], 'LABEL': answer})\n",
        "    print(index)"
      ],
      "metadata": {
        "id": "ieprSLuz1Iru",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "9556f6c3-a33c-43f4-c882-cc7a3eeb6714"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "20\n",
            "21\n",
            "22\n",
            "23\n",
            "24\n",
            "25\n",
            "26\n",
            "27\n",
            "28\n",
            "29\n",
            "30\n",
            "31\n",
            "32\n",
            "33\n",
            "34\n",
            "35\n",
            "36\n",
            "37\n",
            "38\n",
            "39\n",
            "40\n",
            "41\n",
            "42\n",
            "43\n",
            "44\n",
            "45\n",
            "46\n",
            "47\n",
            "48\n",
            "49\n",
            "50\n",
            "51\n",
            "52\n",
            "53\n",
            "54\n",
            "55\n",
            "56\n",
            "57\n",
            "58\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:tornado.access:503 POST /v1beta/models/gemini-pro:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1091.43ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "59\n",
            "60\n",
            "61\n",
            "62\n",
            "63\n",
            "64\n",
            "65\n",
            "66\n",
            "67\n",
            "68\n",
            "69\n",
            "70\n",
            "71\n",
            "72\n",
            "73\n",
            "74\n",
            "75\n",
            "76\n",
            "77\n",
            "78\n",
            "79\n",
            "80\n",
            "81\n",
            "82\n",
            "83\n",
            "84\n",
            "85\n",
            "86\n",
            "87\n",
            "88\n",
            "89\n",
            "90\n",
            "91\n",
            "92\n",
            "93\n",
            "94\n",
            "95\n",
            "96\n",
            "97\n",
            "98\n",
            "99\n",
            "100\n",
            "101\n",
            "102\n",
            "103\n",
            "104\n",
            "105\n",
            "106\n",
            "107\n",
            "108\n",
            "Error Handled \n",
            "109\n",
            "110\n",
            "111\n",
            "112\n",
            "113\n",
            "114\n",
            "115\n",
            "116\n",
            "117\n",
            "118\n",
            "119\n",
            "120\n",
            "121\n",
            "122\n",
            "123\n",
            "124\n",
            "125\n",
            "126\n",
            "127\n",
            "128\n",
            "129\n",
            "130\n",
            "131\n",
            "132\n",
            "133\n",
            "134\n",
            "135\n",
            "136\n",
            "137\n",
            "138\n",
            "139\n",
            "140\n",
            "141\n",
            "142\n",
            "143\n",
            "144\n",
            "145\n",
            "146\n",
            "147\n",
            "148\n",
            "149\n",
            "Error Handled \n",
            "150\n",
            "151\n",
            "152\n",
            "153\n",
            "154\n",
            "155\n",
            "156\n",
            "157\n",
            "158\n",
            "159\n",
            "160\n",
            "161\n",
            "162\n",
            "163\n",
            "164\n",
            "165\n",
            "166\n",
            "167\n",
            "168\n",
            "169\n",
            "170\n",
            "171\n",
            "172\n",
            "173\n",
            "174\n",
            "175\n",
            "176\n",
            "177\n",
            "178\n",
            "179\n",
            "180\n",
            "181\n",
            "182\n",
            "183\n",
            "184\n",
            "185\n",
            "186\n",
            "187\n",
            "188\n",
            "189\n",
            "190\n",
            "191\n",
            "192\n",
            "193\n",
            "194\n",
            "195\n",
            "196\n",
            "197\n",
            "198\n",
            "199\n",
            "200\n",
            "201\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "creating an output file for evaluation"
      ],
      "metadata": {
        "id": "0a65GR_aSV5K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a DataFrame from the output data\n",
        "output_df = pd.DataFrame(output_data)\n",
        "\n",
        "# Write the output DataFrame to a CSV file\n",
        "output_df.to_csv('zero_shot.csv', index=False)"
      ],
      "metadata": {
        "id": "rsDDNR5w1G-r"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}